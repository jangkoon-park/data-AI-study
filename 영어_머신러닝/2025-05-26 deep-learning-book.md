전체 문장:

**Even the human brain does not have an exorbitant amount of connections per neuron.**

---

### 구문 해석:

- **Even**: ~조차도
- **the human brain**: 인간의 뇌
- **does not have**: 가지고 있지 않다
- 🔴**an exorbitant amount of ~**: 과도한 양의 ~
- **connections per neuron**: 뉴런당 연결들

→ 인간의 뇌조차도 뉴런 하나당 과도한 연결을 가지고 있지는 않다.

---

### 문장 구조 트리 (한국어로 설명):

```
[문장]
└── 주어: Even the human brain
    ├── 강조어: Even (~조차도)
    └── 핵심 명사구: the human brain (인간의 뇌)

└── 서술어: does not have
    ├── 조동사: does
    ├── 부정어: not
    └── 동사: have (가지고 있다 → 가지고 있지 않다)

└── 목적어: an exorbitant amount of connections per neuron
    ├── 한정사: an (하나의)
    ├── 형용사구: exorbitant (과도한)
    ├── 명사: amount (양)
    └── 전치사구: of connections per neuron
        ├── 명사: connections (연결들)
        └── 부가 설명: per neuron (뉴런당)

```

---

### 요약된 의미:

**"인간의 뇌조차도 뉴런 하나당 과도한 수의 연결을 가지지 않는다."**

필요하면 다른 문장도 이런 식으로 분석해 줄게.

전체 문장:

**Moreover, deep learning has consistently been applied with success to broader and broader sets of applications.**

---

### 구문 해석:

- **Moreover**: 게다가
- **deep learning**: 딥러닝
- **has consistently been applied**: 지속적으로 적용되어 왔다
- 🔴**with success**: 성공적으로, 성공을 거두며
- **to broader and broader sets of applications**: 점점 더 넓은 범주의 응용 분야에

→ 게다가, 딥러닝은 점점 더 넓은 범주의 응용 분야에 지속적으로 성공적으로 적용되어 왔다.

---

### 문장 구조 트리 (한국어로 설명):

```
[문장]
└── 부사(문장 전체 수식): Moreover (게다가)

└── 주어: deep learning (딥러닝)

└── 서술어: has consistently been applied
    ├── 조동사: has
    ├── 부사: consistently (지속적으로)
    ├── 수동태 동사: been applied (적용되어 왔다)

└── 부사구: with success
    └── 전치사구: with success (성공적으로)

└── 전치사구(목적): to broader and broader sets of applications
    ├── 전치사: to (~에)
    └── 명사구: broader and broader sets of applications
        ├── 반복 형용사: broader and broader (점점 더 넓은)
        └── 명사: sets of applications (응용 분야의 집합들)

```

---

### 요약된 의미:

**"게다가, 딥러닝은 점점 더 넓은 응용 분야에 지속적으로 성공적으로 적용되어 왔다."**

전체 문장:

**The earliest deep models were used to recognize individual objects in tightly cropped, extremely small images (Rumelhart et al., 1986a).**

---

### 구문 해석:

- **The earliest deep models**: 가장 초기의 딥러닝 모델들
- **were used**: 사용되었다 (수동태)
- **to recognize**: 인식하기 위해
- **individual objects**: 개별 물체들을
- **in tightly cropped, extremely small images**: 아주 작고, 꽉 잘려진 이미지 속에서
- **(Rumelhart et al., 1986a)**: 해당 연구 참고 문헌

→ 가장 초기의 딥러닝 모델들은 아주 작고, 꽉 잘려진 이미지 속에서 개별 물체들을 인식하는 데 사용되었다.

---

### 문장 구조 트리 (한국어로 설명):

```
[문장]

└── 주어: The earliest deep models
    ├── 한정사: The (그)
    ├── 최상급 형용사: earliest (가장 초기의)
    ├── 형용사: deep (딥)
    └── 명사: models (모델들)

└── 서술어: were used
    ├── 조동사(be): were
    └── 과거분사: used (사용되었다 – 수동태)

└── 목적(부사적 용도): to recognize individual objects
    ├── 부정사: to recognize (인식하기 위해)
    └── 목적어: individual objects (개별 물체들)

└── 전치사구: in tightly cropped, extremely small images
    ├── 전치사: in (~안에서)
    ├── 형용사구:
        ├── **🔴**tightly cropped (단단히 잘려진)
        └── extremely small (극도로 작은)
    └── 명사: images (이미지들)

└── 참고 문헌: (Rumelhart et al., 1986a)

```

---

### 요약된 의미:

**"가장 초기의 딥러닝 모델들은 극도로 작고 단단히 잘려진 이미지에서 개별 물체들을 인식하기 위해 사용되었다."**

필요하면 다음 문장도 분석해줄게.